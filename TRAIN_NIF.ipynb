{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Train a Neural Image Field in Keras\n",
    "\n",
    "In this tutorial you will learn how to train a simple neural network model using Keras on Graphcloud\n",
    "IPUs. If you have any problems following this guide then please ask questions on the dedicated channel\n",
    "in Graphcore’s Slack Community: https://graphcorecommunity.slack.com.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "First you need to launch an IPU machine: click the 'start machine' button above. You will be prompted to login or create an account (we recommend signing in with Github if you use it). Note that free machines can take up to ten minutes to start up.\n",
    "\n",
    "![Start Machine Screenshot](images/start_machine_poplar.png)\n",
    "\n",
    "Once the machine has launched you can run through the rest of the notebook.\n",
    "\n",
    "## Training a Neural Image Field (NIF)\n",
    "\n",
    "The model we will train is a co-ordinate network for reconstructing/compressing images by a small network of fully connected layers. We refer to this as a neural-image-field (NIF) due to the parallels with [neural radiance fields](https://arxiv.org/abs/2003.08934) (NERF) and the fact there is no consistent term in the literature for it. The model has a simple MLP structure as shown below.\n",
    "\n",
    "![NIF Model Architecture](images/nif/nif_architecture.png)\n",
    "\n",
    "The training inputs are pairs of vectors: the input 2D image coordinates (u, v) (normalised to [0,1)) and the corresponding red, green, blue (r,g,b) value at that position in the image. In effect, the task is to regress a function:\n",
    "\n",
    "$$\n",
    "  f([u,v]) \\rightarrow [r,g,b]\n",
    "$$\n",
    "\n",
    "This function allows the network to reconstruct the entire image (by feeding a batch of inputs that represents coordinates of a regular grid) or to sample the image at any sparse set of image coordinates. If you want to learn more about approximating images with neural networks we suggest referring to the paper [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](https://arxiv.org/abs/2006.10739) which has a good theoretical analysis of the problem.\n",
    "\n",
    "### Clone Graphcore's GitHub Examples Repository\n",
    "\n",
    "Graphcore's examples repository contains many models from its [model garden](https://www.graphcore.ai/resources/model-garden). Run the cell below to clone it and install the example's pip requirements. (Note: you can also launch a terminal using the button on the left and type these commands instead of running them in cells if you prefer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/graphcore/examples.git\n",
    "%cd \"examples/vision/neural_image_fields/tensorflow2\"\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the nif training script on the provided image just to check everything is working. The following will train for a small number of epochs on the example image provided. It should complete within a few minutes (or you can terminate it after a few epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env TF_CPP_MIN_LOG_LEVEL=3\n",
    "%run train_nif.py --train-samples 1000000 --epochs 50 --input Mandrill_portrait_2_Berlin_Zoo.jpg --disable-psnr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check we can reconstruct the original image from the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run predict_nif.py --model saved_model --output reconstruction.jpg --original Mandrill_portrait_2_Berlin_Zoo.jpg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the reconstruction [reconstruction.jpg](examples/vision/neural_image_fields/tensorflow2/reconstruction.jpg) and compare against the original image [Mandrill_portrait_2_Berlin_Zoo.jpg](examples/vision/neural_image_fields/tensorflow2/Mandrill_portrait_2_Berlin_Zoo.jpg).\n",
    "\n",
    "### Train an HDRI Model\n",
    "\n",
    "We now want to train the model for longer with a different type of input. The image used above was a low dynamic range image (i.e. each colour channel is stored in an 8-bit unsigned integer). The image was also compressed with lossy compression (JPG). We will now train the same network with a high-dynamic-range image (HDRI) where each colour channel is stored in 32-bit floating point and the image has been compressed losslessly (if at all). Such images are often used in rendering and later we will use this neural network as part of a simple “neural” ray-tracing program that runs on the IPU (specifically a Monte-Carlo path-tracing program).\n",
    "\n",
    "First, we need to download a suitable image. I am going to use this image for illustration purposes: [polyhaven: studio small 09](https://polyhaven.com/a/studio_small_09). We can download the 2k version directly to our machine and begin training by running this cell, this will take about 10 minutes to train with this configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.polyhaven.org/file/ph-assets/HDRIs/exr/2k/studio_small_09_2k.exr -O studio_small_09_2k.exr\n",
    "%run train_nif.py --train-samples 8000000 --epochs 100 --callback-period 20 --batch-size 1024 --layer-count 6 --layer-size 320 --input studio_small_09_2k.exr --model hdri_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Result\n",
    "\n",
    "Unlike the first training session this one was saving evaluation images as it went. You can download the reconstructed image with this link: [HDRI reconstruction](examples/vision/neural_image_fields/tensorflow2/hdri_model/tmp_eval_image.exr) (or using the file browser on the left). Because this image is in EXR format you will need to use an application that supports high dynamic range images such as [TEV](https://github.com/Tom94/tev/releases) or [pfsview](https://pfstools.sourceforge.net/}). You can also load the image and perform a simple tone-mapping for display in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to apply simple gamma correction, rescale,\n",
    "# and clip values into range 0-255:\n",
    "def gamma_correct(x, exposure, gamma):\n",
    "  scale = 2.0 ** exposure\n",
    "  y = np.power(x * scale, 1.0 / gamma) * 255.0\n",
    "  return np.clip(y, 0.0, 255.0)\n",
    "\n",
    "# Function to plot an opencv image:\n",
    "def display_image(img):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.style.use('dark_background')\n",
    "  plt.imshow(cv2.cvtColor(ldr, cv2.COLOR_BGR2RGB), interpolation='bicubic')\n",
    "  plt.show()\n",
    "\n",
    "EXR_FLAGS = cv2.IMREAD_UNCHANGED | cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH\n",
    "hdr = cv2.imread('hdri_model/tmp_eval_image.exr', EXR_FLAGS)\n",
    "print(f\"HDR image shape: {hdr.shape} type: {hdr.dtype} min: {np.min(hdr)} max: {np.max(hdr)}\")\n",
    "\n",
    "ldr = gamma_correct(hdr, exposure=1.0, gamma=2.4).astype(np.uint8)\n",
    "display_image(ldr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the exposure settings to see that the neural network preserves the high dynamic range of the input image. For example, note the detail in the brightest regions has not been clipped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldr = gamma_correct(hdr, exposure=-3.0, gamma=1.6).astype(np.uint8)\n",
    "display_image(ldr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Training vs GPU\n",
    "\n",
    "Paperspace also provides access to GPUs (inlcuding some on the free tier). You could try and run the same training script on a GPU to compare training time per epoch. To run on GPU you only need to change the following in the training command:\n",
    " - Add `--no-ipu` to the training script options.\n",
    " - Disable parallel PSNR evaluation: `--disable-psnr` (otherwise the script would try to use multiple GPUs which you may not have available).\n",
    "\n",
    "## ACA Workshop\n",
    "\n",
    "If you are running this notebook because you plan to attend an ACA workshop then you can download and save the model folder: `examples/vision/neural_image_fields/tensorflow2/hdri_model`. You could also choose a different image from [Polyhaven](https://polyhaven.com) and build a NIF for that instead if you like. We will load your model for inference inside a \"neural renderer\". The path-tracer, implemented in Poplar C++, will query the neural network to calculate realistic lighting effects with results like this:\n",
    "\n",
    "![Image Rendered with Neural Environment Lighting](images/nif/nif_render.png)\n",
    "\n",
    "Don't worry, a pre-trained model will be provided for those who skipped this tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
